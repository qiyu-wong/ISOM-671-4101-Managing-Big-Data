{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Codes for Setting ups for the rest of our projects on shell\n",
    "\n",
    "#installing packages\n",
    "#sudo python3.7 -m pip install IPython\n",
    "#sudo python3.7 -m pip install pandas\n",
    "#sudo python3.7 -m pip install boto3\n",
    "#sudo python3.7 -m pip install networkx\n",
    "\n",
    "#setting up for graphframes\n",
    "#spark-shell --jars scala-logging_2.12-3.5.0.jar\n",
    "#pyspark --packages graphframes:graphframes:0.8.1-spark2.4-s_2.11\n",
    "#nano ~/.bashrc\n",
    "#export SPARK_OPTS=\"--packages graphframes:graphframes:0.8.1-spark2.4-s_2.11\"\n",
    "#source .bashrc\n",
    "#sc.addPyFile('/home/hadoop/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar')\n",
    "\n",
    "#sudo cp \"/home/hadoop/.ivy2/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar\" \"/usr/lib/spark/jars\"\n",
    "#pyspark --packages graphframes:graphframes:0.8.1-spark2.4-s_2.11 --jars graphframes:0.8.1-spark2.4-s_2.11.jar\n",
    "\n",
    "#kill jobs that are not needed\n",
    "#yarn application -kill application_1605965195748_0042\n",
    "#hadoop job -kill RUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the graphframe packege(the regular installation with pip won't provide the lastest version which matches spark2.4)\n",
    "sc.addPyFile('/usr/lib/spark/jars/graphframes_graphframes-0.8.1-spark2.4-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the other packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "relations = spark.read.option(\"inferSchema\", \"true\").option(\"header\",\n",
    "\"true\").csv(\"s3://smokeeveryday/data420/routes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the layout and partitions\n",
    "relations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimize the partition\n",
    "relations = relations.repartition(col(\"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find all unique nodes and produce a node list\n",
    "node = relations.select(\"src\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = relations.select(\"dst\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "node = unionAll(node,tmp).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = node.withColumnRenamed(\"src\",\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating indegree and outdegree\n",
    "relations = relations.withColumn(\"cnt\",lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = relations.groupBy(relations.src).sum()\n",
    "centrality = centrality.withColumnRenamed(\"sum(cnt)\",\"outDegree\")\n",
    "centrality = centrality.withColumnRenamed(\"src\",\"node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = relations.groupBy(relations.dst).sum()\n",
    "tmp = tmp.withColumnRenamed(\"sum(cnt)\",\"inDegree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = centrality.join(tmp, centrality.node == tmp.dst, \"left\")\n",
    "\n",
    "centrality = centrality.drop(\"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's the manual calculation for page-range without using packages, however, its running slower than graphframes,\n",
    "#so we decide to go with the graphframe\n",
    "\n",
    "#lines = sc.textFile(\"s3://smokeeveryday/data420/Relation.csv\")\n",
    "#header = lines.first()\n",
    "#lines = lines.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#links = lines.map(lambda nodes: nodes.split(\",\")).groupByKey()\n",
    "#def computeContribs(nodes, rank):\n",
    "#    num_nodes = len(nodes)\n",
    "#    for node in nodes:\n",
    "#        yield (node, rank / num_nodes)\n",
    "#from operator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ranks = links.map(lambda nodes: (nodes[0], 1.0))\n",
    "#for iteration in range(50):\n",
    "#    contribs = links.join(ranks).flatMap(lambda nodes:\n",
    "#                                         computeContribs(nodes[1][0], nodes[1][1]))\n",
    "#    ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank*0.85 + 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = spark.createDataFrame(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estalish a graphframe with edge list and node list\n",
    "g = GraphFrame(node, relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate pagerank\n",
    "tmp = g.pageRank(resetProbability=0.15, maxIter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp.vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine with result table\n",
    "centrality = centrality.join(tmp, centrality.node == tmp.id, \"left\")\n",
    "\n",
    "#centrality = centrality.drop(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_range = g.pageRank(resetProbability=0.15, tol=0.1, maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function we wrote to calculate closeness with shortest paths.\n",
    "#However, it works on smaller datasets, but failed on the large edge list we used.\n",
    "#I first thought it was because partition wasn't optimal, so I re-partitioned it, but still not working on large set.\n",
    "\n",
    "#def closeness(g):\n",
    "#    tmp = g.vertices\n",
    "#    tmp = tmp.repartition(col(\"id\"))\n",
    "#    shortestPaths = g.shortestPaths(landmarks = tmp.rdd.map(lambda x: x.id).collect())\n",
    "#    pathLength = shortestPaths.select('id', explode('distances'))\n",
    "#    groupedKey = pathLength.groupBy('key')\n",
    "#    sumOfGroupedDistances = groupedKey.agg(sum('value').alias('c'))\n",
    "#    return sumOfGroupedDistances.selectExpr('key as id','1/c as closeness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#close = closeness(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centrality = centrality.join(close, centrality.node == close.id, \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality = centrality.fillna(0)\n",
    "centrality = centrality.drop(\"id\")\n",
    "centrality.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer result to a dataframe\n",
    "central = centrality.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload the result to a S3 bucket\n",
    "bucket = 'smokeeveryday'\n",
    "csv_buffer = StringIO()\n",
    "central.to_csv(csv_buffer)\n",
    "s3_resource = boto3.resource('s3')\n",
    "s3_resource.Object(bucket, 'central.csv').put(Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
